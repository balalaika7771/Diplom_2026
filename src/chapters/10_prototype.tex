\chapter{Реализация прототипа}

\section{Цели и область применения прототипа}

Прототип реализует архитектуру системы интеллектуальных агентов (глава~8) в виде работающего программного комплекса. Его назначение --- экспериментальная проверка гипотезы (\ref{eq:hypothesis_ineq}): обеспечить уменьшение среднего времени восстановления $\text{MTTR}$ и улучшение метрик $m \in \{\text{MTTR}, \text{precision}, \text{false\_positive\_rate}, \text{diagnosis\_time}\}$ на измеримые величины $I_m$, определённые во введении.

Основные цели:
\begin{itemize}
  \item интегрировать модули сбора метрик, логов и трейсов с промышленными системами наблюдаемости (Prometheus, Elasticsearch, Jaeger);
  \item реализовать цепочку Anomaly Detector $\rightarrow$ Diagnostic Engine $\rightarrow$ Orchestrator $\rightarrow$ Safe Executor;
  \item обеспечить воспроизведение типовых сценариев инцидентов в условиях, близких к production;
  \item подготовить экспериментальную платформу для количественной оценки (глава~11).
\end{itemize}

\section{Технологический стек}

Прототип реализован на Java с использованием Spring Boot \cite{spring_framework}. Стек: DeepLearning4j (LSTM), JGraphT (причинно-следственные графы), Spring WebFlux (реактивное взаимодействие), Kafka (шина событий), Redis (кэш контекста), InfluxDB (хранение метрик), Kubernetes Client (управление кластером). Выбор DeepLearning4j обусловлен требованием единой JVM-экосистемы: все модули агента (Collector, Detector, Diagnostic, Executor) выполняются в JVM, что исключает межпроцессную сериализацию при передаче данных между детектором аномалий и остальными компонентами. Альтернативы (PyTorch, TensorFlow) потребовали бы Python-сервиса с REST/gRPC-мостом, увеличивая латентность инференса на 15--30 мс.

Кластер развёртывания включает:
\begin{itemize}
  \item Kubernetes-кластер с 10 сервисами бизнес-логики (микросервисное приложение e-commerce);
  \item подсистему наблюдаемости: Prometheus, Elasticsearch, Jaeger;
  \item подсистему агентов: Collector, Detector, Diagnostic, Orchestrator, Executor, LLM Agent --- каждый как отдельный \texttt{Deployment}.
\end{itemize}

Взаимодействие компонентов организовано через:
\begin{itemize}
  \item Kafka-топики для передачи событий аномалий и диагнозов;
  \item Redis как кэш контекстной информации;
  \item REST/gRPC-интерфейсы для вызова действий восстановления.
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=0.8cm, auto, scale=0.75, transform shape, font=\small]
    \node[rectangle, draw, fill=blue!20, minimum width=1.2cm, minimum height=0.6cm] (prometheus) {Prometheus};
    \node[rectangle, draw, fill=green!20, below=of prometheus, minimum width=1.2cm, minimum height=0.6cm] (elasticsearch) {Elasticsearch};
    \node[rectangle, draw, fill=yellow!20, below=of elasticsearch, minimum width=1.2cm, minimum height=0.6cm] (jaeger) {Jaeger};
    \node[rectangle, draw, fill=red!20, right=of prometheus, xshift=1cm, minimum width=1.2cm, minimum height=0.6cm] (collector) {Collector};
    \node[rectangle, draw, fill=orange!20, right=of collector, xshift=1cm, minimum width=1.2cm, minimum height=0.6cm] (detector) {Detector};
    \node[rectangle, draw, fill=purple!20, below=of detector, minimum width=1.2cm, minimum height=0.6cm] (diagnostic) {Diagnostic};
    \node[rectangle, draw, fill=cyan!20, below=of collector, minimum width=1.2cm, minimum height=0.6cm] (orchestrator) {Orchestrator};
    \node[rectangle, draw, fill=pink!20, below=of orchestrator, minimum width=1.2cm, minimum height=0.6cm] (executor) {Executor};
    \node[rectangle, draw, fill=brown!20, right=of diagnostic, xshift=1cm, minimum width=1.2cm, minimum height=0.6cm] (llm) {LLM};

    \draw[->, shorten >=2pt, shorten <=2pt] (prometheus) -- (collector);
    \draw[->, shorten >=2pt, shorten <=2pt] (elasticsearch) -- (collector);
    \draw[->, shorten >=2pt, shorten <=2pt] (jaeger) -- (collector);
    \draw[->, shorten >=2pt, shorten <=2pt] (collector) -- (detector);
    \draw[->, shorten >=2pt, shorten <=2pt] (detector) -- (diagnostic);
    \draw[->, shorten >=2pt, shorten <=2pt] (diagnostic) -- (orchestrator);
    \draw[->, shorten >=2pt, shorten <=2pt] (orchestrator) -- (executor);
    \draw[->, shorten >=2pt, shorten <=2pt] (diagnostic) -- (llm);
    \draw[->, shorten >=2pt, shorten <=2pt] (llm) -- (orchestrator);
\end{tikzpicture}
\caption{Архитектура прототипа}
\label{fig:prototype_architecture}
\end{figure}

\section{Реализация модулей}

Каждый модуль прототипа соответствует компоненту архитектуры из главы~8. Ниже описаны особенности реализации, не покрытые формальными моделями.

\subsection{Модуль сбора и нормализации данных}

Сервис \texttt{DataCollectorService} реализует периодический и событийный опрос Prometheus, Elasticsearch и Jaeger, приводя данные к унифицированному формату (\ref{eq:unified_data_model}). \texttt{DataProcessorService} выполняет агрегацию, фильтрацию и нормализацию: усреднение по окнам, downsampling, обогащение контекстом.

\paragraph{Пример унификации.}
Prometheus публикует \texttt{http\_request\_duration\_seconds\{service="checkout"\}} $= 0.8$ c, а Elasticsearch содержит лог уровня ERROR с \texttt{trace\_id}~$= \tau$. Collector преобразует их в записи:
\[
  d_1 = (t, \text{``checkout''}, \text{metric}, 0.8, \{\text{status} = \text{``5xx''}\}),
\]
\[
  d_2 = (t, \text{``checkout''}, \text{log}, \text{``ERROR''}, \{\text{trace\_id} = \tau\}),
\]
что позволяет связывать метрики и логи по полям \(\text{source}\) и \(\text{trace\_id}\) при построении диагностического графа.

\subsection{Модуль обнаружения аномалий}

\texttt{AnomalyDetectorService} реализует алгоритмы, описанные в главе~6: пороговые правила (Z-score), модели ARIMA и многомерную LSTM-модель. Конфигурация LSTM: $w = 60$ точек, $k = 3$ (параметр чувствительности), критерий аномалии $e_t > \mu + k\sigma$.

Результат работы --- события \texttt{AnomalyEvent(service, metric, score, window)}, размещаемые в Kafka-топиках. Например, при $Z(t) = 3.67 > \theta_Z = 3$ для метрики latency сервиса \texttt{checkout} и одновременном LSTM-прогнозе $\hat{\text{Latency}}(t+\Delta t) = 380 > \theta_{\text{degradation}} = 350$ мс формируется событие аномалии, передаваемое в Diagnostic Engine.

\subsection{Модуль диагностического графа}

\texttt{DiagnosticEngineService} строит причинно-следственный граф по формулам~(\ref{eq:causal_probability})--(\ref{eq:root_cause}) с использованием библиотеки JGraphT. На вход поступают метрики, логи и трейсы; на выходе --- \texttt{DiagnosisReport} с ранжированными кандидатами на корневую причину.

\paragraph{Пример.}
Для инцидента с ростом latency \texttt{/checkout} трейс Jaeger фиксирует цепочку
$\text{API-Gateway} \rightarrow \text{CheckoutService} \rightarrow \text{PaymentService} \rightarrow \text{DB}$.
Модуль строит граф, где $P(\text{PaymentService} \to \text{DB}) \approx 0.8$ (высокая корреляция задержек и топология трейса), а $P(\text{CheckoutService} \to \text{PaymentService}) \approx 0.3$. С учётом ошибок 5xx только на уровне \texttt{PaymentService} алгоритм выдаёт $\text{RootCause} = \text{PaymentService}$.

\subsection{Модуль оркестрации и исполнения}

Модуль \texttt{orchestrator} реализует консенсус агентов по формулам~(\ref{eq:consensus})--(\ref{eq:degroot_consensus}). На вход поступают рекомендации от Diagnostic Engine, LLM-агента и rule-based политик; на выходе --- \texttt{RecoveryPlan}.

\texttt{RecoveryExecutorService} преобразует план в операции Kubernetes (масштабирование \texttt{Deployment}, изменение \texttt{ConfigMap}, управление circuit breaker) через Kubernetes Client. Все действия валидируются модулем Safe Executor по критерию~(\ref{eq:safety_criterion}) с порогом $r_{\max} = 0.3$.

\subsection{Модуль LLM Agent}

LLM-агент получает структурированный контекст и формирует: текстовые объяснения инцидента, гипотезы причин и рекомендации по восстановлению. Результаты поступают как в оркестратор, так и в каналы разработчиков (Slack/почта). Интеграция выполняется через Model Context Protocol (раздел~\ref{sec:mcp}).

\section{Интеграция с LLM через Model Context Protocol}
\label{sec:mcp}

MCP --- протокол на основе JSON-RPC~2.0 для стандартизированного взаимодействия LLM-агентов с внешними системами. MCP-сервер выступает прослойкой между LLM-провайдерами и внутренними сервисами прототипа.

\subsection{Реализованные MCP-инструменты}

В прототипе реализовано пять инструментов, каждый из которых соответствует логически целостной операции:

\begin{table}[H]
\centering
\caption{MCP-инструменты прототипа}
\label{tab:mcp_tools}
\small
\begin{tabular}{|p{3cm}|p{8.5cm}|}
\hline
\textbf{Инструмент} & \textbf{Назначение} \\
\hline
\texttt{system\_context} & Агрегированный срез состояния: метрики $M(t)$, сервисы $S(t)$, события $E(t-\Delta t, t)$, алерты $A(t)$ \\
\hline
\texttt{log\_analysis} & Фильтрация ошибок, извлечение паттернов и сущностей (NER), корреляции логов с метриками \\
\hline
\texttt{causal\_graph} & Доступ к диагностическому графу: вершины, рёбра, кандидаты на корневую причину, вероятности \\
\hline
\texttt{recommendations} & Полный цикл: контекст + логи + граф $\to$ LLM $\to$ резюме, причины, рекомендуемые действия \\
\hline
\texttt{action\_result} & Фиксация результата: действие принято/выполнено/откатано/отклонено \\
\hline
\end{tabular}
\end{table}

Обработка каждого запроса реализована как композиция этапов:
\begin{equation}
\text{Process}(R) = \text{Validate}(R) \circ \text{Authenticate}(R) \circ \text{Execute}(R) \circ \text{Format}(R)
\label{eq:mcp_processing}
\end{equation}

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm, auto, scale=0.75, transform shape, font=\small]
    \node[rectangle, draw, fill=blue!20, minimum width=2cm, minimum height=0.8cm] (llm) {LLM-провайдер};
    \node[rectangle, draw, fill=green!20, below=of llm, minimum width=2.5cm, minimum height=0.8cm] (mcp) {MCP-сервер};
    \node[rectangle, draw, fill=yellow!20, below left=of mcp, minimum width=2cm, minimum height=0.8cm] (ctx) {system\_context};
    \node[rectangle, draw, fill=yellow!20, below=of mcp, minimum width=2cm, minimum height=0.8cm] (logs) {log\_analysis};
    \node[rectangle, draw, fill=yellow!20, below right=of mcp, minimum width=2cm, minimum height=0.8cm] (graph) {causal\_graph};

    \draw[->, shorten >=2pt, shorten <=2pt] (llm) -- node[right]{JSON-RPC} (mcp);
    \draw[->, shorten >=2pt, shorten <=2pt] (mcp) -- (ctx);
    \draw[->, shorten >=2pt, shorten <=2pt] (mcp) -- (logs);
    \draw[->, shorten >=2pt, shorten <=2pt] (mcp) -- (graph);
    \draw[->, shorten >=2pt, shorten <=2pt] (ctx) -- (mcp);
    \draw[->, shorten >=2pt, shorten <=2pt] (logs) -- (mcp);
    \draw[->, shorten >=2pt, shorten <=2pt] (graph) -- (mcp);
    \draw[->, shorten >=2pt, shorten <=2pt] (mcp) -- node[left]{Ответ} (llm);
\end{tikzpicture}
\caption{Схема взаимодействия LLM-провайдера с MCP-сервером}
\label{fig:mcp_server}
\end{figure}

Типичный диалог: LLM формирует запрос <<объяснить рост 5xx для \texttt{order-service}>>, MCP-сервер вызывает \texttt{system\_context}, \texttt{log\_analysis} и \texttt{causal\_graph}, агрегирует результаты и возвращает структурированный ответ. Поддерживаются провайдеры: OpenAI (GPT-4), Anthropic (Claude~3), Ollama (Llama~3), Azure OpenAI.

\subsection{Отказоустойчивость MCP-сервера}

Retry с экспоненциальной задержкой:
\begin{equation}
T_{\text{retry}} = T_{\text{base}} \cdot 2^{n-1} \cdot (1 + \text{random}(0, j))
\label{eq:retry_delay}
\end{equation}

Кэширование контекста с TTL, определяемым минимальным интервалом обновления источников:
\begin{equation}
\tau_{\text{cache}} = \min(\Delta t_{\text{metrics}}, \Delta t_{\text{logs}}, \Delta t_{\text{traces}})
\label{eq:cache_ttl}
\end{equation}

\section{Сценарии самовосстановления}

Прототип реализует четыре сценария, моделирующих типовые инциденты в микросервисных системах.

\subsection{Сценарий 1: Высокая загрузка CPU}

Обнаружение: $\text{CPU}(t) > \theta_{\text{CPU}} = 0.9$. Диагностика: идентификация перегруженного сервиса через анализ метрик. Действие --- автомасштабирование:
\begin{equation}
N_{\text{new}} = \left\lceil \frac{\text{CPU}(t)}{\theta_{\text{CPU}}} \cdot N_{\text{current}} \right\rceil
\label{eq:scaling_action}
\end{equation}
Валидация: $\text{CPU}(t+\Delta t) < \theta_{\text{CPU}}$.

\subsection{Сценарий 2: Ошибки соединения}

Обнаружение: $\text{ErrorRate}_{5xx}(t) / \text{TotalRequests}(t) > \theta_{\text{error}} = 0.05$. Диагностика: локализация проблемного сервиса по графу зависимостей. Действие: включение circuit breaker или переключение на резервную зависимость. Валидация: возврат $\text{ErrorRate}$ к допустимому уровню.

\subsection{Сценарий 3: Прогнозируемая деградация}

Система реагирует превентивно на основе LSTM-прогноза. Если $T_{\text{predicted}} - t < \Delta t_{\text{lead}}$, где
\begin{equation}
T_{\text{predicted}} = \arg\min_t \{t : \hat{M}(t) > \theta_{\text{threshold}}\},
\label{eq:degradation_prediction_proto}
\end{equation}
Orchestrator формирует план превентивных действий: увеличение реплик, перераспределение трафика, ужесточение лимитов. Сценарий демонстрирует использование LSTM не только для фиксации факта деградации, но и для упреждающего самовосстановления.

\subsection{Сценарий 4: Неудачный релиз и откат}

После выката новой версии сервиса фиксируется рост $\text{ErrorRate}$ и MTTR. Если $\text{Risk}(a_{\text{keep}}, s_t) > \theta_{\text{risk}}$ по критерию~(\ref{eq:safety_criterion}), Safe Executor инициирует откат к состоянию $s_{t-k}$ по модели~(\ref{eq:rollback_model}), что соответствует применению предыдущих манифестов Kubernetes. После отката модуль верифицирует снижение $\text{ErrorRate}$.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.65]
\begin{axis}[
    xlabel={Время, сек},
    ylabel={CPU, \%},
    title={Сценарий самовосстановления при высокой нагрузке},
    grid=major,
    legend pos=north west,
    width=0.9\textwidth,
    height=6cm,
]
\addplot[blue,thick] coordinates {
    (0, 50) (10, 60) (20, 75) (30, 92) (40, 95) (50, 65) (60, 55)
};
\addplot[red,dashed,thick] coordinates {
    (30, 95) (40, 65)
};
\node at (axis cs:35,80) {\tiny Recovery};
\legend{CPU Usage, Recovery}
\end{axis}
\end{tikzpicture}
\caption{Динамика CPU при автомасштабировании (сценарий~1)}
\label{fig:recovery_scenario}
\end{figure}

\section{Конфигурация и развёртывание}

Параметры системы задаются в \texttt{application.yml} модулей и манифестах Kubernetes. Ключевые параметры:

\begin{table}[H]
\centering
\caption{Основные параметры конфигурации прототипа}
\label{tab:prototype_params}
\small
\begin{tabular}{|p{5cm}|c|p{5cm}|}
\hline
\textbf{Параметр} & \textbf{Значение} & \textbf{Обоснование} \\
\hline
Порог аномалии ($\theta_{\text{anomaly}}$) & 0.85 & Grid search по $F_1$ (глава~11) \\
\hline
Окно LSTM ($w_{\text{LSTM}}$) & 60 & Баланс детализации и вычислительных затрат \\
\hline
Максимум действий за цикл & 5 & Ограничение каскадных изменений \\
\hline
Порог риска ($r_{\max}$) & 0.3 & Экспертная оценка, валидирована в апробации \\
\hline
Порог сходимости консенсуса ($\epsilon$) & 0.01 & Компромисс скорости и точности \\
\hline
\end{tabular}
\end{table}

Деплой выполняется через манифесты Kubernetes с поэтапным обновлением и проверкой health checks. Для каждого сервиса агентов определены профили окружений (локальное развёртывание, тестовый кластер).

\section{Производительность и масштабируемость}

Измеренная производительность прототипа:

\begin{table}[H]
\centering
\caption{Производительность прототипа}
\label{tab:prototype_performance}
\small
\begin{tabular}{|p{6cm}|c|}
\hline
\textbf{Показатель} & \textbf{Значение} \\
\hline
Обработка метрик (DataCollectorService) & $10^5$ метрик/с \\
\hline
Индексация логов (Elasticsearch) & $10^6$ сообщений/мин \\
\hline
Построение графа ($10^4$ вершин) & $\leq 5$ с \\
\hline
LLM-запросы через MCP & до 100 запросов/мин \\
\hline
Задержка Kafka (сбор $\to$ обнаружение) & $< 100$ мс \\
\hline
\end{tabular}
\end{table}

Масштабирование: для 10--50 сервисов --- один экземпляр каждого агента; для 50--500 --- 3--5 экземпляров с балансировкой по Kafka-топикам; для 500+ --- шардирование по ключам \texttt{service}/\texttt{namespace}.

Дискретизация: $\Delta t = 1$ с для метрик, $\Delta t = 100$ мс для трейсов; ошибка аппроксимации $\epsilon < 0.01$. Модульная декомпозиция проверена на отсутствие циклических зависимостей через статический анализ графа Maven.

\section{Тестирование}

Качество реализации подтверждено тремя уровнями тестирования:
\begin{itemize}
  \item \textbf{Unit-тесты} (JUnit~5): покрытие основных сервисов $> 80\%$.
  \item \textbf{Интеграционные тесты}: взаимодействие модулей через Kafka и Redis --- корректность форматов, обработка отказов брокера, повторные попытки.
  \item \textbf{End-to-end тесты}: полный цикл на реальных сценариях инцидентов (сбор данных $\rightarrow$ обнаружение $\rightarrow$ диагностика $\rightarrow$ восстановление $\rightarrow$ верификация).
\end{itemize}

\section{Ограничения прототипа}

Прототип имеет следующие ограничения:

\begin{itemize}
    \item \textbf{Целевая платформа.} Система ориентирована на микросервисные архитектуры с HTTP/gRPC-коммуникациями в Kubernetes. Инциденты на уровне сетевого оборудования (L2/L3), мэйнфреймов и serverless-функций не рассматриваются.

    \item \textbf{Набор действий восстановления.} Реализовано 4 типа действий: масштабирование реплик, перезапуск подов, изменение ConfigMap, управление circuit breaker. Миграция данных, переконфигурация топологии и управление DNS не автоматизированы.

    \item \textbf{Зависимость от LLM-провайдера.} Качество рекомендаций LLM Agent зависит от выбранной модели и её доступности. При недоступности LLM-провайдера (сетевой сбой, rate limit) система продолжает работать без LLM-обогащения, но с потерей 4.6\% $F_1$ (ablation study, глава~11). Стоимость LLM-инференса (0.01--0.03 USD за запрос для GPT-4) не включена в TCO (глава~12).

    \item \textbf{Качество входных данных.} LSTM-детектор калиброван на данных с интервалом scrape 15~с; при увеличении интервала (30~с, 60~с) пороги требуют рекалибровки. Downsampling метрик до 1-минутного разрешения снижает $F_1$ обнаружения на $\sim$8\%.
\end{itemize}
