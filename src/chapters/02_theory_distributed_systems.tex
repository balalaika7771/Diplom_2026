\chapter{Теоретические основы распределённых вычислительных систем}

\section{Введение: что такое распределённая система}

Под распределённой вычислительной системой будем понимать совокупность независимых вычислительных узлов, координированно решающих общую вычислительную задачу и взаимодействующих через коммуникационную сеть \cite{tanenbaum_distributed_systems}. Каждый узел обладает собственной памятью и процессором и не разделяет память с другими узлами, поэтому согласованность их состояний достигается исключительно посредством обмена сообщениями \cite{acm_distributed_systems}. Классическими примерами являются банковские платёжные системы с географически распределёнными процессинг-центрами, крупные интернет-платформы электронной коммерции, а также облачные инфраструктуры, реализующие масштабирование за счёт множества сервисов, развёрнутых в различных дата-центрах.

Основные характеристики распределённых систем:
\begin{itemize}
    \item Независимость узлов: каждый компьютер может работать автономно
    \item Отсутствие общей памяти: узлы не могут напрямую обращаться к памяти друг друга
    \item Отсутствие глобальных часов: у каждого узла своё представление о времени
    \item Возможность отказов: любой узел или канал связи может выйти из строя
\end{itemize}

Эти особенности создают фундаментальные проблемы, которые необходимо решать при проектировании таких систем.

\section{Проблема координации в распределённых системах}

Основная проблема распределённых систем заключается в том, как узлы могут согласованно работать, не имея общей памяти и глобального представления о времени.

Рассмотрим, например, два банкомата в разных городах, которые должны проверить баланс одного и того же счёта. Если они обращаются к независимым копиям базы данных, требуется механизм, гарантирующий согласованность получаемой ими информации. Это частный случай общей проблемы координации в распределённой системе.

Математически распределённую систему можно представить как граф $G = (V, E)$, где $V = \{v_1, v_2, \ldots, v_n\}$ — множество узлов (компьютеров), а $E \subseteq V \times V$ — множество рёбер, представляющих каналы связи между узлами. Например, если у нас есть три сервера, соединённых сетью, то $V = \{\text{сервер1}, \text{сервер2}, \text{сервер3}\}$, а рёбра показывают, какие серверы могут обмениваться данными.

Каждый узел $v_i$ имеет своё локальное состояние $s_i(t)$ в момент времени $t$. Локальное состояние — это вся информация, которой обладает узел: данные в его памяти, текущие выполняемые задачи, настройки и т.д. Глобальное состояние всей системы представляет собой объединение всех локальных состояний:

\begin{equation}
S(t) = (s_1(t), s_2(t), \ldots, s_n(t))
\label{eq:global_state}
\end{equation}

Проблема отсутствия глобальных часов приводит к тому, что разные узлы имеют различное представление о времени. Например, когда узел $v_1$ считает, что сейчас 10:00:00, узел $v_2$ может считать, что сейчас 10:00:03. Эта разница называется дрейфом часов (clock skew). Математически это можно записать так:

\begin{equation}
|t_i - t_j| \leq \delta
\label{eq:clock_drift}
\end{equation}

где $t_i$ — локальное время узла $v_i$, $t_j$ — локальное время узла $v_j$, а $\delta$ — максимально допустимый дрейф часов. Например, если $\delta = 1$ секунда, то разница во времени между любыми двумя узлами не превышает одной секунды.

Эта проблема критична, потому что многие алгоритмы в распределённых системах требуют точного знания времени. Например, чтобы определить, какая транзакция была выполнена раньше, нужно знать точное время её выполнения.

\section{Теорема CAP: фундаментальное ограничение}

Теорема CAP формулирует одно из самых важных ограничений распределённых систем \cite{brewer_cap_theorem}. Она утверждает, что невозможно одновременно обеспечить три свойства: консистентность (Consistency), доступность (Availability) и устойчивость к разделению (Partition tolerance).

Что означают эти свойства?

Консистентность означает, что все узлы системы видят одни и те же данные в один и тот же момент времени. Если вы записали новое значение в базу данных на одном сервере, то при чтении с любого другого сервера вы должны получить это новое значение. Математически это можно сформулировать так: система консистентна, если для любого чтения $R$ в момент времени $t$:

\begin{equation}
R(t) = \text{значение последней записи } W \text{ до момента } t
\label{eq:consistency}
\end{equation}

Доступность означает, что система всегда отвечает на запросы, даже если некоторые узлы вышли из строя. Если пользователь отправляет запрос, система должна обработать его и вернуть ответ, пусть даже не самый свежий. Доступность определяется как вероятность успешного выполнения запроса:

\begin{equation}
A = \lim_{T \to \infty} \frac{\text{количество успешных запросов за } T}{\text{общее количество запросов за } T}
\label{eq:availability}
\end{equation}

Например, если из 1000 запросов 999 были успешно обработаны, то доступность составляет 99.9\%.

Устойчивость к разделению означает, что система продолжает работать даже при разделении сети на непересекающиеся части. Например, если из-за сбоя сети серверы в Москве потеряли связь с серверами в Санкт-Петербурге, система должна продолжать работать в обеих частях.

Теорема CAP утверждает, что нельзя обеспечить все три свойства одновременно:

\begin{equation}
\text{CAP} \Rightarrow \neg(\text{C} \land \text{A} \land \text{P})
\label{eq:cap_theorem}
\end{equation}

Почему это так? Рассмотрим сценарий разделения сети. Пусть сеть разделена на два множества узлов $V_1$ и $V_2$, которые не могут общаться друг с другом. Если узел в $V_1$ выполняет запись данных, а узел в $V_2$ пытается прочитать эти данные, возникает конфликт:

\begin{itemize}
    \item Если система обеспечивает консистентность и устойчивость к разделению, она должна заблокировать чтение до восстановления связи между $V_1$ и $V_2$. Но это нарушает доступность — система не может ответить на запрос.
    \item Если система обеспечивает доступность и устойчивость к разделению, она должна разрешить чтение, но узел в $V_2$ получит старое значение, что нарушает консистентность.
\end{itemize}

Поэтому при проектировании распределённых систем необходимо выбирать, какое из свойств пожертвовать. Например, банковские системы обычно выбирают консистентность и устойчивость к разделению, жертвуя доступностью — лучше временно не обрабатывать транзакции, чем допустить ошибки в данных.

Расширение теоремы CAP — теорема PACELC — учитывает ещё один важный аспект: задержку (Latency) и консистентность в нормальных условиях (когда сеть не разделена) \cite{abadi_pacelc}. Она утверждает:

\begin{equation}
\text{PACELC}: \text{если разделение, то выбор между A и C; иначе выбор между L и C}
\label{eq:pacelc}
\end{equation}

Это означает, что даже когда сеть работает нормально, существует компромисс между задержкой ответа и консистентностью данных. Этот компромисс формализуется следующим образом:

\begin{equation}
L = f(C, N, R)
\label{eq:latency_consistency}
\end{equation}

где $L$ — задержка ответа, $C$ — уровень консистентности, $N$ — количество копий данных (реплик), $R$ — количество узлов, которые должны подтвердить операцию.

Для строгой консистентности требуется синхронизация всех реплик, что увеличивает задержку:

\begin{equation}
L_{\text{strong}} = 2 \cdot \max_{i,j} d(v_i, v_j) + T_{\text{processing}}
\label{eq:strong_consistency_latency}
\end{equation}

где $d(v_i, v_j)$ — задержка передачи данных между узлами $v_i$ и $v_j$, а $T_{\text{processing}}$ — время обработки запроса. Коэффициент 2 появляется потому, что нужно отправить запрос на все реплики и получить от них подтверждения.

Для eventual consistency (консистентности в конечном счёте) задержка определяется только локальной обработкой, так как не требуется ждать подтверждения от других узлов:

\begin{equation}
L_{\text{eventual}} = T_{\text{local\_processing}}
\label{eq:eventual_consistency_latency}
\end{equation}

Это означает, что система может быстро ответить пользователю, но данные на разных узлах могут временно различаться, пока не произойдёт синхронизация.

\section{Микросервисная архитектура: решение проблемы масштабирования}

Традиционная монолитная архитектура, где вся система представляет собой единое приложение, плохо масштабируется при росте нагрузки. Например, в интернет-магазине увеличение числа пользователей приводит к необходимости вертикального масштабирования монолита (усиления единого сервера), что экономически неэффективно и быстро упирается в аппаратные ограничения.

Микросервисная архитектура решает эту проблему через декомпозицию системы на независимые сервисы \cite{newman_microservices,richardson_microservices_patterns,ieee_microservices}. Вместо одного большого приложения создаётся множество маленьких сервисов, каждый из которых отвечает за свою функцию. Например, один сервис обрабатывает авторизацию пользователей, другой — каталог товаров, третий — корзину покупок.

Математически система из $n$ микросервисов представляется как множество:

\begin{equation}
S = \{s_1, s_2, \ldots, s_n\}, \quad s_i \cap s_j = \emptyset \text{ для } i \neq j
\label{eq:microservices_set}
\end{equation}

Условие $s_i \cap s_j = \emptyset$ означает, что сервисы не пересекаются — каждый сервис имеет свою область ответственности и не дублирует функциональность других.

Граф зависимостей сервисов $G_D = (S, E_D)$ показывает, какие сервисы зависят от каких. Рёбро $(s_i, s_j) \in E_D$ означает, что сервис $s_i$ использует функциональность сервиса $s_j$. Например, сервис корзины покупок зависит от сервиса каталога товаров, чтобы получить информацию о товарах.

Метрика связанности системы показывает, насколько тесно связаны сервисы:

\begin{equation}
\text{Coupling}(S) = \frac{|E_D|}{n(n-1)/2} \in [0, 1]
\label{eq:coupling_metric}
\end{equation}

Знаменатель $n(n-1)/2$ — это максимально возможное количество связей между $n$ сервисами. Если $\text{Coupling}(S) = 1$, это означает, что каждый сервис зависит от всех остальных, что плохо для масштабирования. Для микросервисной архитектуры $\text{Coupling}(S) \ll 1$, то есть сервисы слабо связаны, что позволяет масштабировать их независимо.

Пропускная способность системы определяется самым медленным сервисом (узким местом):

\begin{equation}
\text{Throughput}(S) = \min_{s_i \in S} \text{Throughput}(s_i)
\label{eq:system_throughput}
\end{equation}

Например, если сервис авторизации может обработать 1000 запросов в секунду, а сервис каталога — только 500, то общая пропускная способность системы составит 500 запросов в секунду.

Преимущество микросервисной архитектуры в том, что можно масштабировать только тот сервис, который является узким местом. Если сервис каталога не справляется с нагрузкой, можно добавить дополнительные экземпляры этого сервиса, не трогая другие. Масштабирование отдельного сервиса $s_i$ с коэффициентом $k_i$:

\begin{equation}
\text{Throughput}(s_i') = k_i \cdot \text{Throughput}(s_i)
\label{eq:service_scaling}
\end{equation}

Например, если добавить ещё один экземпляр сервиса каталога ($k_i = 2$), его пропускная способность удвоится. Общая пропускная способность системы после масштабирования:

\begin{equation}
\text{Throughput}(S') = \min_{s_i \in S} k_i \cdot \text{Throughput}(s_i)
\label{eq:scaled_throughput}
\end{equation}

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2cm]
    \node[rectangle, draw, fill=blue!20, minimum width=2cm, minimum height=1cm] (api) {API Gateway};
    \node[rectangle, draw, fill=green!20, minimum width=2cm, minimum height=1cm, below left=of api] (auth) {Auth Service};
    \node[rectangle, draw, fill=green!20, minimum width=2cm, minimum height=1cm, below=of api] (order) {Order Service};
    \node[rectangle, draw, fill=green!20, minimum width=2cm, minimum height=1cm, below right=of api] (payment) {Payment Service};
    \node[rectangle, draw, fill=yellow!20, minimum width=2cm, minimum height=1cm, below=of order] (db1) {DB 1};
    \node[rectangle, draw, fill=yellow!20, minimum width=2cm, minimum height=1cm, below=of payment] (db2) {DB 2};
    
    \draw[->] (api) -- (auth);
    \draw[->] (api) -- (order);
    \draw[->] (api) -- (payment);
    \draw[->] (order) -- (db1);
    \draw[->] (payment) -- (db2);
\end{tikzpicture}
\caption{Архитектура микросервисной системы с графом зависимостей}
\label{fig:microservices_architecture}
\end{figure}

\section{Паттерны коммуникации: проблема задержек}

В распределённых системах задержка коммуникации между узлами является критическим фактором. Представьте ситуацию: пользователь нажимает кнопку "Купить" в интернет-магазине, и система должна проверить наличие товара, списать деньги со счёта и оформить заказ. Если каждый из этих шагов требует обращения к другому серверу, общая задержка может быть значительной.

Синхронная коммуникация означает, что отправитель запроса блокируется до получения ответа и не может продолжать выполнение до завершения удалённой операции. Время синхронной коммуникации определяется как:

\begin{equation}
T_{\text{sync}} = T_{\text{request}} + T_{\text{network}} + T_{\text{process}} + T_{\text{network}} + T_{\text{response}}
\label{eq:sync_latency}
\end{equation}

где $T_{\text{request}}$ — время формирования запроса, $T_{\text{network}}$ — сетевая задержка (время передачи данных по сети), $T_{\text{process}}$ — время обработки запроса на сервере, $T_{\text{response}}$ — время формирования ответа. Сетевая задержка учитывается дважды: один раз при отправке запроса и один раз при получении ответа.

Для цепочки из $n$ синхронных вызовов (например, сервис A вызывает сервис B, который вызывает сервис C) общая задержка складывается:

\begin{equation}
T_{\text{chain}} = \sum_{i=1}^{n} T_{\text{sync},i}
\label{eq:chain_latency}
\end{equation}

Проблема в том, что при большом количестве вызовов задержка растёт линейно, что делает систему медленной. Например, если каждый вызов занимает 100 миллисекунд, а цепочка состоит из 10 вызовов, общая задержка составит 1 секунду.

Решение этой проблемы — асинхронная коммуникация через очереди сообщений. Вместо того чтобы ожидать немедленного ответа, отправитель помещает сообщение в очередь и продолжает выполнение собственной логики. Получатель извлекает сообщение из очереди, обрабатывает его и при необходимости формирует ответ, помещая его в отдельную очередь.

Модель очереди сообщений основана на теории массового обслуживания. Рассмотрим простейшую модель M/M/1 (Markovian arrivals — случайные поступления, Markovian service — случайное обслуживание, один сервер). Для стабильной работы очереди необходимо, чтобы интенсивность поступления сообщений $\lambda$ была меньше интенсивности обслуживания $\mu$:

\begin{equation}
\rho = \frac{\lambda}{\mu} < 1
\label{eq:queue_utilization}
\end{equation}

Параметр $\rho$ называется коэффициентом загрузки. Если $\rho \geq 1$, очередь будет расти бесконечно, так как сообщения поступают быстрее, чем обрабатываются.

Средняя длина очереди (количество сообщений, ожидающих обработки):

\begin{equation}
L = \frac{\rho}{1-\rho} = \frac{\lambda}{\mu - \lambda}
\label{eq:queue_length}
\end{equation}

Например, если $\lambda = 80$ сообщений в секунду, а $\mu = 100$ сообщений в секунду, то $\rho = 0.8$ и средняя длина очереди $L = 0.8/(1-0.8) = 4$ сообщения.

Среднее время ожидания в очереди определяется формулой Литтла:

\begin{equation}
W = \frac{L}{\lambda} = \frac{1}{\mu - \lambda}
\label{eq:waiting_time}
\end{equation}

В нашем примере среднее время ожидания составит $W = 4/80 = 0.05$ секунды, то есть 50 миллисекунд.

Пропускная способность асинхронной системы:

\begin{equation}
\text{Throughput}_{\text{async}} = \min(\lambda, \mu) = \begin{cases}
\lambda & \text{если } \rho < 1 \\
\mu & \text{если } \rho \geq 1
\end{cases}
\label{eq:async_throughput}
\end{equation}

Преимущество асинхронной коммуникации в том, что отправитель не блокируется и может обрабатывать другие запросы, пока сообщение обрабатывается. Это значительно повышает общую производительность системы.

\section{Деградация и отказы: проблема надёжности}

Распределённые системы подвержены отказам компонентов \cite{ieee_distributed_systems,self_healing_systems,ieee_self_healing}. Отказ может произойти по разным причинам: сбой оборудования, ошибка в программном обеспечении, перегрузка сети и т.д. Для оценки надёжности системы используется вероятностный подход \cite{floyd_algorithms}.

Вероятность отказа узла $v_i$ в интервале времени $[0, t]$ описывается экспоненциальным распределением:

\begin{equation}
F_i(t) = 1 - R_i(t) = 1 - e^{-\lambda_i t}
\label{eq:failure_probability}
\end{equation}

где $R_i(t)$ — функция надёжности узла (вероятность того, что узел проработает без отказа до момента $t$), а $\lambda_i$ — интенсивность отказов (failure rate). Интенсивность отказов показывает, сколько отказов происходит в единицу времени. Например, если $\lambda_i = 0.001$ отказов в час, это означает, что в среднем узел отказывает один раз в 1000 часов.

Для системы из $n$ последовательно соединённых узлов (система работает только если все узлы работают) надёжность вычисляется как произведение надёжностей отдельных узлов:

\begin{equation}
R_{\text{series}}(t) = \prod_{i=1}^{n} R_i(t) = \prod_{i=1}^{n} e^{-\lambda_i t} = e^{-\sum_{i=1}^{n} \lambda_i t}
\label{eq:series_reliability}
\end{equation}

Например, если система состоит из трёх узлов с надёжностью каждого 0.9, то общая надёжность составит $0.9 \times 0.9 \times 0.9 = 0.729$, то есть 72.9\%. Это показывает, что надёжность последовательной системы быстро снижается с увеличением количества компонентов.

Интенсивность отказов последовательной системы равна сумме интенсивностей отказов отдельных узлов:

\begin{equation}
\lambda_{\text{series}} = \sum_{i=1}^{n} \lambda_i
\label{eq:series_failure_rate}
\end{equation}

Для системы с параллельным резервированием (система работает если работает хотя бы один узел) надёжность вычисляется иначе:

\begin{equation}
R_{\text{parallel}}(t) = 1 - \prod_{i=1}^{n} (1 - R_i(t)) = 1 - \prod_{i=1}^{n} (1 - e^{-\lambda_i t})
\label{eq:parallel_reliability}
\end{equation}

Для $n$ идентичных узлов с $\lambda_i = \lambda$:

\begin{equation}
R_{\text{parallel}}(t) = 1 - (1 - e^{-\lambda t})^n
\label{eq:parallel_reliability_identical}
\end{equation}

Например, если два идентичных узла имеют надёжность 0.9 каждый, то надёжность параллельной системы составит $1 - (1-0.9)^2 = 1 - 0.01 = 0.99$, то есть 99\%. Это показывает преимущество резервирования.

Среднее время между отказами (MTBF — Mean Time Between Failures) определяется как \cite{ieee_distributed_systems,self_healing_systems}:

\begin{equation}
\text{MTBF} = \int_0^{\infty} R(t) dt = \frac{1}{\lambda}
\label{eq:mtbf}
\end{equation}

Для экспоненциального распределения MTBF просто равно обратной величине интенсивности отказов. Например, если $\lambda = 0.001$ отказов в час, то MTBF = 1000 часов.

Среднее время до восстановления (MTTR — Mean Time To Repair) показывает, сколько времени в среднем требуется для восстановления системы после отказа \cite{pagerduty_incident_response,google_sre_book}:

\begin{equation}
\text{MTTR} = \int_0^{\infty} t \cdot f_{\text{repair}}(t) dt
\label{eq:mttr}
\end{equation}

где $f_{\text{repair}}(t)$ — функция плотности вероятности времени восстановления.

Доступность системы (вероятность того, что система работает в произвольный момент времени) определяется через MTBF и MTTR \cite{iso_25010,google_sre_book}:

\begin{equation}
A = \frac{\text{MTBF}}{\text{MTBF} + \text{MTTR}} = \frac{1/\lambda}{1/\lambda + \text{MTTR}} = \frac{1}{1 + \lambda \cdot \text{MTTR}}
\label{eq:availability_formula}
\end{equation}

Например, если MTBF = 1000 часов, а MTTR = 10 часов, то доступность составит $1000/(1000+10) = 0.99$, то есть 99\%. Для повышения доступности необходимо либо снижать интенсивность отказов $\lambda$ (улучшать качество оборудования и программного обеспечения), либо уменьшать время восстановления MTTR (быстрее обнаруживать и устранять проблемы).

Каскадные отказы возникают, когда отказ одного компонента вызывает отказы других. Например, если один сервер перегружен и перестаёт отвечать, нагрузка перераспределяется на другие серверы, которые также могут перегрузиться. Вероятность распространения отказов определяется следующим образом:

\begin{equation}
P(\text{failure}|v_i \text{ failed}) = \sum_{v_j \in \text{neighbors}(v_i)} w_{ij} \cdot P(\text{failure}|v_j)
\label{eq:cascade_failure}
\end{equation}

где $w_{ij}$ — вес влияния узла $v_j$ на узел $v_i$, а $\text{neighbors}(v_i)$ — соседи узла $v_i$ в графе зависимостей.

Для предотвращения каскадных отказов применяется изоляция (bulkhead pattern) — разделение системы на независимые части:

\begin{equation}
\text{Isolation}(v_i) = \{v_j : d(v_i, v_j) > d_{\text{threshold}}\}
\label{eq:isolation}
\end{equation}

где $d(v_i, v_j)$ — расстояние между узлами в графе зависимостей, $d_{\text{threshold}}$ — порог изоляции. Узлы, находящиеся дальше порога, изолируются от узла $v_i$, что предотвращает распространение отказов.

Рассмотренные свойства распределённых систем — координация без общей памяти и глобальных часов, ограничения CAP/PACELC, микросервисная декомпозиция, характеристики синхронной и асинхронной коммуникации, модели надёжности и каскадных отказов — задают формальное описание их поведения. При заданных требованиях к консистентности, доступности, задержке и надёжности выбор топологии, схем репликации, взаимодействий и резервирования сводится к выбору допустимых значений этих параметров.

В дальнейших главах эти свойства используются как исходные допущения при построении методов диагностики, прогнозирования деградации и самовосстановления, а также при обосновании архитектуры интеллектуальных агентов, работающих поверх наблюдаемости и эксплуатационных метрик.
