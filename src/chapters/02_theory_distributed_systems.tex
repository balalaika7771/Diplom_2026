\chapter{Теоретические основы распределённых вычислительных систем}

\section{Модель распределённой системы}

Распределённая вычислительная система --- совокупность независимых узлов, координированно решающих общую задачу через обмен сообщениями \cite{tanenbaum_distributed_systems,acm_distributed_systems}. Каждый узел обладает собственной памятью и процессором; глобальное состояние недоступно ни одному из участников напрямую.

Система моделируется графом $G = (V, E)$, где $V = \{v_1, \ldots, v_n\}$ --- узлы, $E \subseteq V \times V$ --- каналы связи. Глобальное состояние:
\begin{equation}
S(t) = (s_1(t), s_2(t), \ldots, s_n(t))
\label{eq:global_state}
\end{equation}
где $s_i(t)$ --- локальное состояние узла $v_i$.

Отсутствие глобальных часов приводит к дрейфу $|t_i - t_j| \leq \delta$, что делает невозможным точное определение порядка событий без дополнительных протоколов. Для автономных агентов это означает, что причинно-следственный анализ инцидентов (глава~8) не может опираться на абсолютные временные метки и должен использовать причинное упорядочение событий.

\section{Ограничения CAP и PACELC}

Теорема CAP \cite{brewer_cap_theorem} устанавливает, что при разделении сети ($P$) невозможно одновременно обеспечить консистентность ($C$) и доступность ($A$):
\begin{equation}
\text{Partition} \Rightarrow \neg(C \land A)
\label{eq:cap_theorem}
\end{equation}

Расширение PACELC \cite{abadi_pacelc} добавляет компромисс в нормальном режиме: даже без разделения существует выбор между задержкой ($L$) и консистентностью ($C$):
\begin{equation}
\text{PACELC}: \text{если } P, \text{ то } A/C; \text{ иначе } L/C
\label{eq:pacelc}
\end{equation}

\begin{table}[H]
\centering
\caption{Стратегии компромисса CAP/PACELC в контексте задач агента}
\label{tab:cap_strategies}
\small
\begin{tabular}{|p{2.5cm}|p{2cm}|p{2.5cm}|p{4cm}|}
\hline
\textbf{Стратегия} & \textbf{Приоритет} & \textbf{Пример} & \textbf{Значение для агента} \\
\hline
CP (Consistency + Partition) & Корректность & ZooKeeper, etcd & Агент может доверять данным конфигурации, но должен учитывать недоступность \\
\hline
AP (Availability + Partition) & Доступность & Cassandra, DynamoDB & Агент должен учитывать eventual consistency при диагностике \\
\hline
EL (Else Latency) & Скорость & Кеши, read-реплики & Агент получает данные быстро, но они могут быть устаревшими \\
\hline
\end{tabular}
\end{table}

Выбор стратегии определяет, какие аномалии агент должен уметь различать: расхождение реплик (AP-системы) --- нормальное поведение, а не инцидент; временная недоступность (CP-системы) --- ожидаемая реакция на разделение.

\section{Микросервисная декомпозиция}

Декомпозиция на микросервисы \cite{newman_microservices,richardson_microservices_patterns,ieee_microservices} позволяет масштабировать компоненты независимо, но создаёт граф зависимостей $G_D = (S, E_D)$, который определяет пути распространения отказов.

Связанность графа зависимостей:
\begin{equation}
\text{Coupling}(S) = \frac{|E_D|}{n(n-1)/2} \in [0, 1]
\label{eq:coupling_metric}
\end{equation}

При $\text{Coupling}(S) \ll 1$ отказ одного сервиса затрагивает ограниченное множество зависимых компонентов, что упрощает локализацию. При высокой связанности каскадные отказы распространяются быстрее, чем агент может диагностировать причину --- это определяет требование к скорости реакции системы (глава~8, MTTR $< 30$ мин для каскадных сценариев).

Пропускная способность системы ограничена узким местом:
\begin{equation}
\text{Throughput}(S) = \min_{s_i \in S} k_i \cdot \text{Throughput}(s_i)
\label{eq:system_throughput}
\end{equation}
где $k_i$ --- коэффициент масштабирования сервиса $s_i$. Агент использует эту зависимость при выборе действия восстановления: масштабирование узкого места эффективнее масштабирования остальных сервисов.

\section{Модели надёжности и каскадные отказы}

Надёжность узла описывается экспоненциальной моделью \cite{ieee_distributed_systems,self_healing_systems}:
\begin{equation}
R_i(t) = e^{-\lambda_i t}, \quad \text{MTBF}_i = \frac{1}{\lambda_i}
\label{eq:failure_probability}
\end{equation}

Для $n$ последовательно зависимых сервисов надёжность падает экспоненциально:
\begin{equation}
R_{\text{series}}(t) = \prod_{i=1}^{n} e^{-\lambda_i t} = e^{-\sum_{i=1}^{n} \lambda_i t}
\label{eq:series_reliability}
\end{equation}

Доступность системы \cite{iso_25010,google_sre_book}:
\begin{equation}
A = \frac{\text{MTBF}}{\text{MTBF} + \text{MTTR}}
\label{eq:availability_formula}
\end{equation}

Из~(\ref{eq:availability_formula}) следует, что при фиксированной интенсивности отказов единственный путь повышения доступности --- снижение MTTR. Именно этот путь --- автоматизация диагностики и восстановления --- является предметом настоящего исследования.

Каскадные отказы описываются через вероятность распространения:
\begin{equation}
P(\text{failure}|v_i \text{ failed}) = \sum_{v_j \in \text{neighbors}(v_i)} w_{ij} \cdot P(\text{failure}|v_j)
\label{eq:cascade_failure}
\end{equation}
где $w_{ij}$ --- вес влияния узла $v_j$ на $v_i$ в графе зависимостей. Эта модель мотивирует построение причинно-следственного графа в модуле Diagnostic Engine (глава~8): направление рёбер в графе зависимостей определяет порядок поиска корневой причины.

\medskip

Рассмотренные свойства --- отсутствие глобального состояния, ограничения CAP/PACELC, графы зависимостей микросервисов, модели надёжности и каскадных отказов --- задают формальные ограничения, в рамках которых работают автономные агенты. Алгоритмы диагностики опираются на эти модели при построении причинных графов и оценке рисков восстановительных действий.
