\chapter{Методы обнаружения аномалий и прогнозирования деградации}

\section{Введение: что такое аномалия и почему её важно обнаруживать}

Под аномалией в распределённых вычислительных системах будем понимать статистически значимое отклонение наблюдаемого поведения от некоторого эталонного (нормального) режима \cite{anomaly_detection_survey,chandola_anomaly}. Такое отклонение может проявляться как резкое изменение уровня нагрузки, ошибок или задержек, так и как изменение структуры корреляций и временных паттернов в данных наблюдаемости.

Современные распределённые системы генерируют большие объёмы гетерогенных данных: метрики производительности (загрузка процессора, использование памяти, интенсивность запросов), логи событий, трейсы распределённых запросов. Традиционные пороговые методы мониторинга, опирающиеся на фиксированные границы для отдельных метрик, не учитывают нестационарность нормального поведения и многомерный характер зависимостей между показателями, что приводит к высокой доле ложных срабатываний и пропуску сложных аномальных паттернов.

Основные проблемы традиционных методов:
\begin{itemize}
    \item Статические пороги не учитывают изменения в нормальном поведении системы
    \item Аномалии могут проявляться не только как отклонения отдельных метрик, но и как изменения паттернов во временных рядах
    \item Корреляции между метриками могут указывать на проблемы, которые не видны при анализе отдельных метрик
    \item Ручная настройка порогов требует экспертных знаний и времени
    \item Ложные срабатывания создают усталость от предупреждений
\end{itemize}

В данной главе рассматриваются математические модели для автоматического обнаружения аномалий и прогнозирования деградации системы.

\section{Проблема обнаружения аномалий в распределённых системах}

Основная проблема заключается в том, что аномалии могут проявляться не только как отклонения отдельных метрик, но и как изменения паттернов во временных рядах или корреляций между метриками. Кроме того, нормальное поведение системы может изменяться со временем, что требует адаптивных методов обнаружения.

Формализуем задачу обнаружения аномалий. Пусть $\mathbf{x} \in \mathbb{R}^d$ — вектор наблюдений (метрики, признаки из логов, характеристики трейсов). Необходимо определить функцию обнаружения:

\begin{equation}
f: \mathbb{R}^d \to \{0, 1\}, \quad f(\mathbf{x}) = \begin{cases}
1 & \text{если } \mathbf{x} \text{ является аномалией} \\
0 & \text{иначе}
\end{cases}
\label{eq:anomaly_detection_function}
\end{equation}

Проблема усложняется тем, что в распределённых системах нормальное поведение может изменяться со временем, что требует адаптивных методов.

\section{Статистические методы: анализ распределений}

Простейший подход основан на предположении о нормальном распределении метрик \cite{hastie_elements,murphy_machine_learning}. Если метрика $X$ следует нормальному распределению $X \sim \mathcal{N}(\mu, \sigma^2)$, то функция плотности вероятности:

\begin{equation}
f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\label{eq:normal_distribution}
\end{equation}

где $\mu = \mathbb{E}[X]$ — математическое ожидание, $\sigma^2 = \mathbb{V}[X]$ — дисперсия.

Для обнаружения аномалий используется стандартизация:

\begin{equation}
z = \frac{x - \mu}{\sigma} = \frac{x - \mathbb{E}[X]}{\sqrt{\mathbb{V}[X]}}
\label{eq:z_score}
\end{equation}

Стандартизированная переменная $z$ следует стандартному нормальному распределению $Z \sim \mathcal{N}(0, 1)$ с функцией плотности:

\begin{equation}
\phi(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2}{2}\right)
\label{eq:standard_normal}
\end{equation}

Критерий аномальности на основе правила трёх сигм:

\begin{equation}
|z| > 3 \Rightarrow P(|Z| > 3) = 2\Phi(-3) \approx 0.0027
\label{eq:three_sigma_rule}
\end{equation}

где $\Phi(z) = \int_{-\infty}^{z} \phi(t) dt$ — функция распределения стандартной нормальной величины.

Однако этот метод имеет ограничения: он предполагает нормальность распределения и не учитывает временные зависимости. Для временных рядов метрик требуется более сложный анализ.

\section{Временные ряды: проблема нестационарности}

Метрики распределённых систем образуют временные ряды $\{y_t\}_{t=1}^{T}$, которые часто нестационарны \cite{time_series_anomaly,acm_anomaly_detection}. Нестационарность проявляется в наличии трендов, сезонности и изменяющейся дисперсии.

Аддитивная модель декомпозиции временного ряда:

\begin{equation}
y_t = T_t + S_t + R_t + \epsilon_t
\label{eq:time_series_decomposition}
\end{equation}

где $T_t$ — трендовая компонента, $S_t$ — сезонная компонента, $R_t$ — циклическая компонента, $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ — белый шум.

Для стационаризации ряда применяется дифференцирование. Ряд $y_t$ называется интегрированным порядка $d$ (обозначается $I(d)$), если $d$-я разность стационарна:

\begin{equation}
\Delta^d y_t = (1-B)^d y_t \sim I(0)
\label{eq:differencing}
\end{equation}

где $B$ — оператор сдвига назад: $By_t = y_{t-1}$.

\subsection{ARIMA модель для прогнозирования}

ARIMA$(p,d,q)$ модель объединяет авторегрессию (AR), интегрирование (I) и скользящее среднее (MA) \cite{hastie_elements,murphy_machine_learning}:

\begin{equation}
(1 - \sum_{i=1}^{p} \phi_i B^i)(1-B)^d y_t = (1 + \sum_{i=1}^{q} \theta_i B^i)\epsilon_t
\label{eq:arima}
\end{equation}

где $p$ — порядок авторегрессии, $d$ — порядок дифференцирования, $q$ — порядок скользящего среднего, $\phi_i, \theta_i$ — параметры модели.

Автокорреляционная функция (ACF) для стационарного процесса:

\begin{equation}
\rho(k) = \frac{\mathbb{E}[(y_t - \mu)(y_{t-k} - \mu)]}{\sigma^2} = \frac{\gamma(k)}{\gamma(0)}
\label{eq:acf}
\end{equation}

где $\gamma(k) = \mathbb{E}[(y_t - \mu)(y_{t-k} - \mu)]$ — автоковариационная функция с лагом $k$.

Частная автокорреляционная функция (PACF) измеряет корреляцию между $y_t$ и $y_{t-k}$ при исключении влияния промежуточных значений:

\begin{equation}
\phi_{kk} = \text{corr}(y_t, y_{t-k} | y_{t-1}, \ldots, y_{t-k+1})
\label{eq:pacf}
\end{equation}

Параметры ARIMA модели оцениваются методом максимального правдоподобия:

\begin{equation}
\hat{\phi}, \hat{\theta} = \arg\max_{\phi, \theta} \prod_{t=1}^{T} f(y_t | y_{t-1}, \ldots, y_1; \phi, \theta)
\label{eq:mle_estimation}
\end{equation}

Прогноз на $h$ шагов вперёд:

\begin{equation}
\hat{y}_{T+h} = \mathbb{E}[y_{T+h} | y_1, \ldots, y_T]
\label{eq:arima_forecast}
\end{equation}

Аномалия обнаруживается, если фактическое значение значительно отклоняется от прогноза:

\begin{equation}
|y_{T+h} - \hat{y}_{T+h}| > k \cdot \sigma_{\text{forecast}}
\label{eq:arima_anomaly}
\end{equation}

где $\sigma_{\text{forecast}}$ — стандартное отклонение ошибки прогноза, $k$ — параметр чувствительности.

\section{Машинное обучение: проблема многомерности}

Методы машинного обучения позволяют обнаруживать аномалии в многомерных данных без предположений о распределении \cite{bishop_pattern_recognition,murphy_machine_learning,ieee_anomaly_detection}. Проблема заключается в том, что аномалии могут проявляться не только как отклонения отдельных признаков, но и как изменения паттернов в многомерном пространстве.

Для многомерных данных статистические методы становятся неэффективными из-за проклятия размерности. Необходимо использовать методы машинного обучения, способные выявлять сложные нелинейные зависимости.

\subsection{Изолирующий лес: решение проблемы изоляции аномалий}

Изолирующий лес (Isolation Forest) основан на идее, что аномалии легче изолировать, чем нормальные точки \cite{isolation_forest}. Алгоритм использует случайные деревья решений для оценки аномальности.

Пусть $T_i(x)$ — глубина изоляции точки $x$ в дереве $i$. Средняя глубина изоляции по ансамблю из $N$ деревьев вычисляется как:

\begin{equation}
E[h(x)] = \frac{1}{N} \sum_{i=1}^{N} T_i(x)
\label{eq:avg_isolation_depth}
\end{equation}

Нормализующая константа для набора из $n$ точек:

\begin{equation}
c(n) = 2H(n-1) - \frac{2(n-1)}{n} \approx 2(\ln(n-1) + \gamma) - \frac{2(n-1)}{n}
\label{eq:isolation_normalization}
\end{equation}

где $H(k) = \sum_{i=1}^{k} \frac{1}{i}$ — гармоническое число, $\gamma \approx 0.577$ — постоянная Эйлера-Маскерони.

Оценка аномальности:

\begin{equation}
s(x, n) = 2^{-\frac{E[h(x)]}{c(n)}}
\label{eq:isolation_forest}
\end{equation}

Интерпретация: если $s(x, n) \approx 1$, точка легко изолируется (аномалия), если $s(x, n) \approx 0.5$ — нормальная точка.

Вероятностная интерпретация:

\begin{equation}
P(\text{anomaly}|x) = \begin{cases}
1 & \text{если } s(x, n) > \theta \\
0 & \text{иначе}
\end{cases}
\label{eq:anomaly_probability}
\end{equation}

где $\theta$ — пороговое значение, обычно $\theta = 0.5$.

\subsection{LSTM: решение проблемы долгосрочных зависимостей}

Для временных рядов с долгосрочными зависимостями обычные рекуррентные сети страдают от проблемы затухающих градиентов \cite{goodfellow_deep_learning}. LSTM решает эту проблему через механизм ячеек с воротами \cite{lstm_time_series}.

Ячейка LSTM состоит из нескольких компонентов. Забывающий гейт определяет, какую информацию удалить из состояния ячейки:

\begin{equation}
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\label{eq:lstm_forget}
\end{equation}

Гейт входа определяет, какую новую информацию сохранить:

\begin{equation}
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\label{eq:lstm_input}
\end{equation}

Кандидат на обновление состояния:

\begin{equation}
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
\label{eq:lstm_candidate}
\end{equation}

Обновление состояния ячейки:

\begin{equation}
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\label{eq:lstm_cell_state}
\end{equation}

где $\odot$ — поэлементное умножение (произведение Адамара).

Гейт выхода определяет, какую часть состояния ячейки использовать для выхода:

\begin{equation}
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\label{eq:lstm_output}
\end{equation}

Скрытое состояние:

\begin{equation}
h_t = o_t \odot \tanh(C_t)
\label{eq:lstm_hidden}
\end{equation}

где $\sigma(x) = \frac{1}{1+e^{-x}}$ — сигмоидальная функция активации, $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ — гиперболический тангенс.

Предсказание на следующем шаге:

\begin{equation}
\hat{x}_{t+1} = W_y \cdot h_t + b_y
\label{eq:lstm_prediction}
\end{equation}

Функция потерь для обучения:

\begin{equation}
L = \frac{1}{T} \sum_{t=1}^{T} (x_t - \hat{x}_t)^2 + \lambda \sum_{W} \|W\|_2^2
\label{eq:lstm_loss}
\end{equation}

где второй член — регуляризация L2 для предотвращения переобучения, $\lambda$ — параметр регуляризации.

Оценка аномальности на основе ошибки предсказания:

\begin{equation}
\text{Anomaly Score}(x_t) = \begin{cases}
1 & \text{если } |e_t| > \mu_e + k\sigma_e \\
0 & \text{иначе}
\end{cases}
\label{eq:lstm_anomaly}
\end{equation}

где $e_t = x_t - \hat{x}_t$ — ошибка предсказания, $\mu_e$ и $\sigma_e$ — среднее и стандартное отклонение ошибок на обучающей выборке, $k$ — параметр чувствительности (обычно $k = 2$ или $k = 3$).

\section{Прогнозирование деградации: проблема предотвращения инцидентов}

Предотвращение инцидентов требует прогнозирования деградации до того, как система достигнет критического состояния. Для этого используется анализ трендов и корреляций метрик.

Вектор метрик в момент времени $t$:

\begin{equation}
\mathbf{m}(t) = [m_1(t), m_2(t), \ldots, m_n(t)]^T \in \mathbb{R}^n
\label{eq:metrics_vector}
\end{equation}

Скорость изменения метрик (первая производная):

\begin{equation}
\frac{d\mathbf{m}(t)}{dt} = \mathbf{v}(t) = \lim_{\Delta t \to 0} \frac{\mathbf{m}(t+\Delta t) - \mathbf{m}(t)}{\Delta t}
\label{eq:metrics_velocity}
\end{equation}

Ускорение изменения метрик (вторая производная):

\begin{equation}
\frac{d^2\mathbf{m}(t)}{dt^2} = \mathbf{a}(t) = \frac{d\mathbf{v}(t)}{dt}
\label{eq:metrics_acceleration}
\end{equation}

Вероятность деградации на основе логистической регрессии \cite{hastie_elements,bishop_pattern_recognition}:

\begin{multline}
P(\text{degradation}|t+\Delta t) = \sigma\left(\sum_{i=1}^{n} w_i \cdot \frac{dm_i(t)}{dt} + w_0\right) \\
= \frac{1}{1 + \exp\left(-\sum_{i=1}^{n} w_i \cdot \frac{dm_i(t)}{dt} - w_0\right)}
\label{eq:degradation_prediction}
\end{multline}

где $\sigma(x) = \frac{1}{1+e^{-x}}$ — сигмоидальная функция, $w_i$ — веса метрик, $w_0$ — смещение.

Многомерная модель прогнозирования с учётом корреляций:

\begin{equation}
\mathbf{\hat{m}}(t+\Delta t) = \mathbf{A} \mathbf{m}(t) + \mathbf{b} + \mathbf{\epsilon}
\label{eq:multivariate_forecast}
\end{equation}

где $\mathbf{A} \in \mathbb{R}^{n \times n}$ — матрица переходов, $\mathbf{b} \in \mathbb{R}^n$ — вектор смещения, $\mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{\Sigma})$ — вектор ошибок с ковариационной матрицей $\mathbf{\Sigma}$.

Корреляционная матрица метрик:

\begin{equation}
\mathbf{R} = \begin{bmatrix}
\rho_{11} & \rho_{12} & \cdots & \rho_{1n} \\
\rho_{21} & \rho_{22} & \cdots & \rho_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\rho_{n1} & \rho_{n2} & \cdots & \rho_{nn}
\end{bmatrix}
\label{eq:correlation_matrix}
\end{equation}

где $\rho_{ij} = \frac{\text{cov}(m_i, m_j)}{\sigma_i \sigma_j}$ — коэффициент корреляции Пирсона между метриками $m_i$ и $m_j$.

Критерий деградации:

\begin{equation}
\text{Degradation}(t+\Delta t) =
  \begin{cases}
    1, & \text{если } \exists i: \hat{m}_i(t+\Delta t) > \theta_i \\
       & \quad \text{или } P(\text{degradation} \mid t+\Delta t) > \theta_p, \\
    0, & \text{иначе}.
  \end{cases}
\label{eq:degradation_criterion}
\end{equation}

где $\theta_i$ — пороговое значение для метрики $m_i$, $\theta_p$ — порог вероятности деградации.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.65]
\begin{axis}[
    xlabel={Время, мин},
    ylabel={Latency, мс},
    title={Прогнозирование деградации},
    grid=major,
    legend pos=north west,
    width=0.9\textwidth,
    height=6cm,
]
\addplot[blue,thick] coordinates {
    (0, 50) (1, 52) (2, 55) (3, 58) (4, 62) (5, 68) (6, 75) (7, 85)
};
\addplot[red,dashed,thick] coordinates {
    (7, 85) (8, 95) (9, 110) (10, 130)
};
\addplot[green,dotted,thick] coordinates {
    (0, 100) (10, 100)
};
\node at (axis cs:5,105) {\tiny Порог};
\legend{Фактические, Прогноз, Порог}
\end{axis}
\end{tikzpicture}
\caption{Прогнозирование деградации производительности}
\label{fig:degradation_prediction}
\end{figure}

На рисунке \ref{fig:degradation_prediction} показано, как модель прогнозирует деградацию до достижения критического порога, что позволяет принять превентивные меры.

\section{LLM-based анализ логов: решение проблемы неструктурированных данных}

Традиционные методы обнаружения аномалий работают с числовыми метриками и структурированными данными. Однако значительная часть информации о проблемах в распределённых системах содержится в текстовых логах: сообщения об ошибках, стектрейсы, описания исключений. Large Language Models (LLM) — большие языковые модели — способны понимать естественный язык и извлекать структурированную информацию из неструктурированных текстовых логов, что делает их мощным инструментом для анализа и помощи разработчикам в диагностике проблем.

LLM представляет собой функцию, которая преобразует последовательность токенов в распределение вероятностей над следующим токеном:

\begin{equation}
P(w_{t+1} | w_1, w_2, \ldots, w_t, \theta) = \text{LLM}(w_1, w_2, \ldots, w_t; \theta)
\label{eq:llm_next_token_logs}
\end{equation}

где $w_i$ — токены входной последовательности, $\theta$ — параметры модели.

Для анализа логов LLM используется в режиме извлечения информации (information extraction). Пусть $L = \{l_1, l_2, \ldots, l_n\}$ — множество логов, каждый лог $l_i$ представляет собой последовательность токенов. LLM извлекает структурированную информацию:

\begin{equation}
\text{StructuredInfo}(L) = \{(e_j, t_j, p_j) : e_j \in E, t_j \in T, p_j \in [0, 1]\}
\label{eq:llm_extraction}
\end{equation}

где $E$ — множество извлечённых сущностей (имена сервисов, типы ошибок, коды исключений), $T$ — множество типов сущностей, $p_j$ — уверенность в извлечении.

Вероятность того, что лог $l_i$ содержит информацию о проблеме:

\begin{equation}
P(\text{problem}|l_i) = \text{LLM}(l_i, \text{prompt}_{\text{problem\_detection}})
\label{eq:llm_problem_probability}
\end{equation}

где $\text{prompt}_{\text{problem\_detection}}$ — промпт для обнаружения проблем в логах.

Для помощи разработчикам LLM генерирует объяснения проблем на естественном языке:

\begin{equation}
\text{Explanation} = \text{LLM}(\text{Logs}, \text{Metrics}, \text{Traces}, \text{prompt}_{\text{explanation}})
\label{eq:llm_explanation}
\end{equation}

Объяснение включает:
\begin{itemize}
    \item Описание проблемы на естественном языке
    \item Вероятные причины проблемы на основе анализа логов
    \item Предлагаемые действия для исправления
    \item Ссылки на релевантные части кода или документации
\end{itemize}

Качество объяснения оценивается через следующую метрику:

\begin{equation}
\text{Quality}(E) = \alpha \cdot \text{Accuracy}(E) + \beta \cdot \text{Clarity}(E) + \gamma \cdot \text{Actionability}(E)
\label{eq:explanation_quality}
\end{equation}

где $\text{Accuracy}$ — точность описания проблемы, $\text{Clarity}$ — ясность объяснения, $\text{Actionability}$ — выполнимость рекомендаций, $\alpha, \beta, \gamma$ — веса компонентов.

Время генерации объяснения:

\begin{equation}
T_{\text{explanation}} = T_{\text{preprocessing}} + T_{\text{llm\_inference}} + T_{\text{postprocessing}}
\label{eq:explanation_time}
\end{equation}

где $T_{\text{preprocessing}}$ — время подготовки данных для LLM (фильтрация релевантных логов, форматирование), $T_{\text{llm\_inference}}$ — время инференса LLM, $T_{\text{postprocessing}}$ — время обработки ответа (валидация, форматирование).

Преимущество LLM-based анализа заключается в способности понимать контекст и семантику текста, что позволяет обнаруживать проблемы, которые не видны при анализе отдельных метрик. Например, LLM может связать сообщение об ошибке в логах с конкретным изменением в коде или конфигурации, что помогает разработчикам быстрее найти и исправить проблему.

Интеграция LLM с традиционными методами:

\begin{equation}
\text{Anomaly Score} =
  w_{\text{metrics}} \cdot \text{Score}_{\text{metrics}}
  + w_{\text{logs}} \cdot \text{Score}_{\text{llm\_logs}}
  + w_{\text{traces}} \cdot \text{Score}_{\text{traces}}
\label{eq:llm_integration}
\end{equation}

где $\text{Score}_{\text{llm\_logs}}$ — оценка аномальности на основе анализа логов через LLM, $w_{\text{metrics}}, w_{\text{logs}}, w_{\text{traces}}$ — веса различных источников данных.

\section{Выбор метода: оптимизация компромисса}

Выбор метода обнаружения аномалий представляет собой задачу оптимизации компромисса между точностью, вычислительной сложностью и интерпретируемостью.

Целевая функция для выбора метода:

\begin{equation}
J(\text{method}) = \alpha_1 \cdot \text{Accuracy} + \alpha_2 \cdot \frac{1}{\text{Complexity}} + \alpha_3 \cdot \text{Interpretability}
\label{eq:method_selection}
\end{equation}

где $\alpha_1 + \alpha_2 + \alpha_3 = 1$ — весовые коэффициенты, отражающие приоритеты системы.

Для систем реального времени важна низкая задержка:

\begin{equation}
\text{Latency}(\text{method}) = T_{\text{preprocessing}} + T_{\text{inference}} + T_{\text{postprocessing}} < T_{\text{max}}
\label{eq:latency_constraint}
\end{equation}

где $T_{\text{max}}$ — максимально допустимая задержка.

Для систем с ограниченными ресурсами важна вычислительная сложность:

\begin{equation}
\text{Complexity}(\text{method}) = O(f(n, d))
\label{eq:complexity}
\end{equation}

где $n$ — количество наблюдений, $d$ — размерность данных, $f$ — функция сложности метода.

Рассматриваемые подходы образуют иерархию методов анализа: от одномерных статистических критериев до многомерных моделей временных рядов, глубоких нейронных сетей и средств обработки неструктурированных логов. Статистические и ARIMA-подходы задают базовый уровень количественной оценки отклонений, тогда как Isolation Forest и LSTM расширяют анализ на многомерные и нелинейные зависимости, используя исторические данные для построения эталонного поведения.

Прогнозирование деградации на основе производных и корреляций метрик переводит задачу из реактивного в превентивный режим, когда решения принимаются до достижения критических порогов эксплуатационных показателей. LLM-ориентированный анализ логов дополняет численные методы: работает с текстовыми описаниями ошибок и извлекает из них структурированную диагностическую информацию.

В совокупности эти методы образуют базу для модуля обнаружения аномалий и прогнозирования деградации в системе интеллектуальных агентов: далее они используются как подстановочные алгоритмы внутри Anomaly Analyzer и диагностических компонентов, а выбор комбинации методов формализуется как задача оптимизации с учётом требований к точности, задержке и интерпретируемости.
